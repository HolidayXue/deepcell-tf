{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Nuclear Segmentation with Mask-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (5760, 216, 256, 1)\n",
      "y.shape: (5760, 216, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Download the data (saves to ~/.keras/datasets)\n",
    "filename = 'HeLa_S3.npz'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.hela_s3.load_data(filename)\n",
    "\n",
    "print('X.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up filepath constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the data file is currently required for `train_model_()` functions\n",
    "\n",
    "# NOTE: Change DATA_DIR if you are not using `deepcell.datasets`\n",
    "DATA_DIR = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "\n",
    "DATA_FILE = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "# confirm the data file is available\n",
    "assert os.path.isfile(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "\n",
    "# If the data file is in a subdirectory, mirror it in MODEL_DIR and LOG_DIR\n",
    "PREFIX = os.path.relpath(os.path.dirname(DATA_FILE), DATA_DIR)\n",
    "\n",
    "ROOT_DIR = '/data'  # TODO: Change this! Usually a mounted volume\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))\n",
    "\n",
    "# create directories if they do not exist\n",
    "for d in (MODEL_DIR, LOG_DIR):\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "model_name = 'mrcnn_model'\n",
    "backbone = 'vgg16'  # vgg16, vgg19, resnet50, densenet121, densenet169, densenet201\n",
    "\n",
    "n_epoch = 10  # Number of training epochs\n",
    "test_size = .20  # % of data saved as test\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "\n",
    "batch_size = 4  # loss can only handle one batch at a time.\n",
    "\n",
    "num_classes = 1  # \"object\" is the only class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the MaskRCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 4) (?, ?, 1)\n",
      "(?, 256) (?, 256, 4) (?, 256, 1)\n",
      "(?, 256, 4) (?, 256, 1) (?, 256, 14, 14, 256)\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "model = model_zoo.MaskRCNN(\n",
    "    backbone=backbone,\n",
    "    input_shape=X_train.shape[1:],\n",
    "    class_specific_filter=True,\n",
    "    num_classes=num_classes)\n",
    "\n",
    "prediction_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from deepcell import losses\n",
    "\n",
    "sigma = 3.0\n",
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "\n",
    "def regress_loss(y_true, y_pred):\n",
    "    # separate target and state\n",
    "    regression = y_pred\n",
    "    regression_target = y_true[..., :-1]\n",
    "    anchor_state = y_true[..., -1]\n",
    "\n",
    "    # filter out \"ignore\" anchors\n",
    "    indices = tf.where(K.equal(anchor_state, 1))\n",
    "    regression = tf.gather_nd(regression, indices)\n",
    "    regression_target = tf.gather_nd(regression_target, indices)\n",
    "\n",
    "    # compute the loss\n",
    "    loss = losses.smooth_l1(regression_target, regression, sigma=sigma)\n",
    "\n",
    "    # compute the normalizer: the number of positive anchors\n",
    "    normalizer = K.maximum(1, K.shape(indices)[0])\n",
    "    normalizer = K.cast(normalizer, dtype=K.floatx())\n",
    "\n",
    "    return K.sum(loss) / normalizer\n",
    "\n",
    "def classification_loss(y_true, y_pred):\n",
    "    # TODO: try weighted_categorical_crossentropy\n",
    "    labels = y_true[..., :-1]\n",
    "    # -1 for ignore, 0 for background, 1 for object\n",
    "    anchor_state = y_true[..., -1]\n",
    "\n",
    "    classification = y_pred\n",
    "    # filter out \"ignore\" anchors\n",
    "    indices = tf.where(K.not_equal(anchor_state, -1))\n",
    "    labels = tf.gather_nd(labels, indices)\n",
    "    classification = tf.gather_nd(classification, indices)\n",
    "\n",
    "    # compute the loss\n",
    "    loss = losses.focal(labels, classification, alpha=alpha, gamma=gamma)\n",
    "\n",
    "    # compute the normalizer: the number of positive anchors\n",
    "    normalizer = tf.where(K.equal(anchor_state, 1))\n",
    "    normalizer = K.cast(K.shape(normalizer)[0], K.floatx())\n",
    "    normalizer = K.maximum(K.cast_to_floatx(1.0), normalizer)\n",
    "\n",
    "    return K.sum(loss) / normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import overlap\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "\n",
    "def mask_loss(iou_threshold=0.5, mask_size=(28, 28)):\n",
    "    def _mask_conditional(y_true, y_pred):\n",
    "        # if there are no masks annotations, return 0; else, compute the masks loss\n",
    "        loss = tf.cond(\n",
    "            K.any(K.equal(K.shape(y_true), 0)),\n",
    "            lambda: K.cast_to_floatx(0.0),\n",
    "            lambda: _mask(y_true, y_pred, iou_threshold=iou_threshold, mask_size=mask_size)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def _mask(y_true, y_pred, iou_threshold=0.5, mask_size=(28, 28)):\n",
    "        # split up the different predicted blobs\n",
    "        boxes = y_pred[:, :, :4]\n",
    "        masks = y_pred[:, :, 4:]\n",
    "\n",
    "        # split up the different blobs\n",
    "        annotations  = y_true[:, :, :5]\n",
    "        width        = K.cast(y_true[0, 0, 5], dtype='int32')\n",
    "        height       = K.cast(y_true[0, 0, 6], dtype='int32')\n",
    "        masks_target = y_true[:, :, 7:]\n",
    "\n",
    "        # reshape the masks back to their original size\n",
    "        masks_target = K.reshape(masks_target, (K.shape(masks_target)[0]* K.shape(masks_target)[1], height, width))\n",
    "        masks        = K.reshape(masks, (K.shape(masks)[0]* K.shape(masks)[1], mask_size[0], mask_size[1], -1))\n",
    "\n",
    "        # TODO: Fix batch_size > 1\n",
    "        boxes        = K.reshape(boxes, (-1, K.shape(boxes)[2]))\n",
    "        annotations  = K.reshape(annotations, (-1, K.shape(annotations)[2]))\n",
    "\n",
    "        # compute overlap of boxes with annotations\n",
    "        iou                  = overlap(boxes, annotations)\n",
    "        argmax_overlaps_inds = K.argmax(iou, axis=1)\n",
    "        max_iou              = K.max(iou, axis=1)\n",
    "\n",
    "        # filter those with IoU > 0.5\n",
    "        indices              = tf.where(K.greater_equal(max_iou, iou_threshold))\n",
    "        boxes                = tf.gather_nd(boxes, indices)\n",
    "        masks                = tf.gather_nd(masks, indices)\n",
    "        argmax_overlaps_inds = K.cast(tf.gather_nd(argmax_overlaps_inds, indices), 'int32')\n",
    "        labels               = K.cast(K.gather(annotations[:, 4], argmax_overlaps_inds), 'int32')\n",
    "\n",
    "        # make normalized boxes\n",
    "        x1 = boxes[:, 0]\n",
    "        y1 = boxes[:, 1]\n",
    "        x2 = boxes[:, 2]\n",
    "        y2 = boxes[:, 3]\n",
    "        boxes = K.stack([\n",
    "            y1 / (K.cast(height, dtype=K.floatx()) - 1),\n",
    "            x1 / (K.cast(width, dtype=K.floatx()) - 1),\n",
    "            (y2 - 1) / (K.cast(height, dtype=K.floatx()) - 1),\n",
    "            (x2 - 1) / (K.cast(width, dtype=K.floatx()) - 1),\n",
    "        ], axis=1)\n",
    "\n",
    "        # crop and resize masks_target\n",
    "        masks_target = K.expand_dims(masks_target, axis=3)  # append a fake channel dimension\n",
    "        masks_target = tf.image.crop_and_resize(\n",
    "            masks_target,\n",
    "            boxes,\n",
    "            argmax_overlaps_inds,\n",
    "            mask_size\n",
    "        )\n",
    "        masks_target = masks_target[:, :, :, 0]  # remove fake channel dimension\n",
    "\n",
    "        # gather the predicted masks using the annotation label\n",
    "        masks = tf.transpose(masks, (0, 3, 1, 2))\n",
    "        label_indices = K.stack([tf.range(K.shape(labels)[0]), labels], axis=1)\n",
    "        masks = tf.gather_nd(masks, label_indices)\n",
    "\n",
    "        # compute mask loss\n",
    "        mask_loss  = K.binary_crossentropy(masks_target, masks)\n",
    "        normalizer = K.shape(masks)[0] * K.shape(masks)[1] * K.shape(masks)[2]\n",
    "        normalizer = K.maximum(K.cast(normalizer, K.floatx()), 1)\n",
    "        mask_loss  = K.sum(mask_loss) / normalizer\n",
    "\n",
    "        return mask_loss\n",
    "\n",
    "    return _mask_conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"filtered_detections\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"filtered_detections\".\n",
      "WARNING:tensorflow:Output \"filtered_detections\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"filtered_detections\".\n",
      "WARNING:tensorflow:Output \"filtered_detections\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"filtered_detections\".\n",
      "WARNING:tensorflow:Output \"filtered_detections\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"filtered_detections\".\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss={\n",
    "    'regression': regress_loss,\n",
    "    'classification': classification_loss,\n",
    "    'boxes_masks': mask_loss()\n",
    "}, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import image_generators\n",
    "\n",
    "datagen = image_generators.RetinaNetGenerator(\n",
    "#     fill_mode='constant',  # for rotations\n",
    "    rotation_range=180,\n",
    "    shear_range=0,\n",
    "    zoom_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen_val = image_generators.RetinaNetGenerator(\n",
    "#     fill_mode='constant',  # for rotations\n",
    "    rotation_range=0,\n",
    "    shear_range=0,\n",
    "    zoom_range=0,\n",
    "    horizontal_flip=0,\n",
    "    vertical_flip=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Removing 14 of 100 images with fewer than 3 objects.\n",
      "WARNING:tensorflow:Removing 11 of 100 images with fewer than 3 objects.\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import make_shapes_callback\n",
    "from deepcell.utils.retinanet_anchor_utils import guess_shapes\n",
    "\n",
    "if 'vgg' in backbone or 'densenet' in backbone:\n",
    "    compute_shapes = make_shapes_callback(model)\n",
    "else:\n",
    "    compute_shapes = guess_shapes\n",
    "\n",
    "train_data = datagen.flow(\n",
    "    {'X': X_train[0:100], 'y': y_train[0:100]},\n",
    "    include_masks=True,\n",
    "    compute_shapes=compute_shapes,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_data = datagen_val.flow(\n",
    "    {'X': X_test[0:100], 'y': y_test[0:100]},\n",
    "    include_masks=True,\n",
    "    compute_shapes=compute_shapes,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    train_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import callbacks\n",
    "from deepcell.callbacks import *\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(\n",
    "    log_dir=os.path.join(LOG_DIR, model_name))\n",
    "\n",
    "training_callbacks = [\n",
    "    callbacks.LearningRateScheduler(lr_sched),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        os.path.join(MODEL_DIR, '{}.h5'.format(model_name)),\n",
    "        monitor='val_loss', verbose=1,\n",
    "        save_best_only=True, save_weights_only=False),\n",
    "    tensorboard_callback,\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss', factor=0.1,\n",
    "        patience=10, verbose=1,\n",
    "        mode='auto', min_delta=0.0001,\n",
    "        cooldown=0, min_lr=0),\n",
    "    RedirectModel(\n",
    "        Evaluate(val_data,\n",
    "                 iou_threshold=0.5,\n",
    "                 score_threshold=0.01,\n",
    "                 max_detections=100,\n",
    "                 tensorboard=tensorboard_callback,\n",
    "                 weighted_average=True),\n",
    "        prediction_model),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/21 [===========================>..] - ETA: 1s - loss: 4.5316 - regression_loss: 2.7106 - classification_loss: 1.1282 - boxes_masks_loss: 0.6928\n",
      "Epoch 00001: val_loss improved from inf to 4.45860, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0000\n",
      "mAP: 0.0000\n",
      "21/21 [==============================] - 33s 2s/step - loss: 4.5301 - regression_loss: 2.7091 - classification_loss: 1.1282 - boxes_masks_loss: 0.6928 - val_loss: 4.4586 - val_regression_loss: 2.6392 - val_classification_loss: 1.1276 - val_boxes_masks_loss: 0.6918\n",
      "Epoch 2/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 4.4795 - regression_loss: 2.6658 - classification_loss: 1.1253 - boxes_masks_loss: 0.6884\n",
      "Epoch 00002: val_loss improved from 4.45860 to 4.37955, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0000\n",
      "mAP: 0.0000\n",
      "21/21 [==============================] - 15s 693ms/step - loss: 4.4749 - regression_loss: 2.6621 - classification_loss: 1.1250 - boxes_masks_loss: 0.6879 - val_loss: 4.3796 - val_regression_loss: 2.5922 - val_classification_loss: 1.1161 - val_boxes_masks_loss: 0.6712\n",
      "Epoch 3/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 4.2968 - regression_loss: 2.5565 - classification_loss: 1.0753 - boxes_masks_loss: 0.6650\n",
      "Epoch 00003: val_loss improved from 4.37955 to 3.91207, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0280\n",
      "mAP: 0.0280\n",
      "21/21 [==============================] - 15s 707ms/step - loss: 4.2834 - regression_loss: 2.5488 - classification_loss: 1.0710 - boxes_masks_loss: 0.6637 - val_loss: 3.9121 - val_regression_loss: 2.3575 - val_classification_loss: 0.9142 - val_boxes_masks_loss: 0.6403\n",
      "Epoch 4/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.9076 - regression_loss: 2.4919 - classification_loss: 0.7851 - boxes_masks_loss: 0.6306\n",
      "Epoch 00004: val_loss improved from 3.91207 to 3.59317, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0392\n",
      "mAP: 0.0392\n",
      "21/21 [==============================] - 15s 717ms/step - loss: 3.8982 - regression_loss: 2.4892 - classification_loss: 0.7772 - boxes_masks_loss: 0.6318 - val_loss: 3.5932 - val_regression_loss: 2.2970 - val_classification_loss: 0.6799 - val_boxes_masks_loss: 0.6163\n",
      "Epoch 5/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.6765 - regression_loss: 2.3922 - classification_loss: 0.6532 - boxes_masks_loss: 0.6312\n",
      "Epoch 00005: val_loss improved from 3.59317 to 3.47925, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0389\n",
      "mAP: 0.0389\n",
      "21/21 [==============================] - 15s 738ms/step - loss: 3.6749 - regression_loss: 2.3951 - classification_loss: 0.6488 - boxes_masks_loss: 0.6310 - val_loss: 3.4792 - val_regression_loss: 2.2761 - val_classification_loss: 0.5911 - val_boxes_masks_loss: 0.6120\n",
      "Epoch 6/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.6340 - regression_loss: 2.3959 - classification_loss: 0.6191 - boxes_masks_loss: 0.6190\n",
      "Epoch 00006: val_loss improved from 3.47925 to 3.43374, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0421\n",
      "mAP: 0.0421\n",
      "21/21 [==============================] - 15s 734ms/step - loss: 3.6462 - regression_loss: 2.4034 - classification_loss: 0.6236 - boxes_masks_loss: 0.6191 - val_loss: 3.4337 - val_regression_loss: 2.2642 - val_classification_loss: 0.5690 - val_boxes_masks_loss: 0.6005\n",
      "Epoch 7/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.5799 - regression_loss: 2.3972 - classification_loss: 0.5766 - boxes_masks_loss: 0.6060\n",
      "Epoch 00007: val_loss improved from 3.43374 to 3.38496, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0427\n",
      "mAP: 0.0427\n",
      "21/21 [==============================] - 16s 741ms/step - loss: 3.5789 - regression_loss: 2.3951 - classification_loss: 0.5786 - boxes_masks_loss: 0.6052 - val_loss: 3.3850 - val_regression_loss: 2.2518 - val_classification_loss: 0.5511 - val_boxes_masks_loss: 0.5821\n",
      "Epoch 8/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.4869 - regression_loss: 2.3353 - classification_loss: 0.5783 - boxes_masks_loss: 0.5734\n",
      "Epoch 00008: val_loss improved from 3.38496 to 3.36663, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0405\n",
      "mAP: 0.0405\n",
      "21/21 [==============================] - 16s 751ms/step - loss: 3.4852 - regression_loss: 2.3345 - classification_loss: 0.5768 - boxes_masks_loss: 0.5739 - val_loss: 3.3666 - val_regression_loss: 2.2285 - val_classification_loss: 0.5699 - val_boxes_masks_loss: 0.5683\n",
      "Epoch 9/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.4802 - regression_loss: 2.3312 - classification_loss: 0.5734 - boxes_masks_loss: 0.5755\n",
      "Epoch 00009: val_loss improved from 3.36663 to 3.33488, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0346\n",
      "mAP: 0.0346\n",
      "21/21 [==============================] - 16s 763ms/step - loss: 3.4790 - regression_loss: 2.3315 - classification_loss: 0.5730 - boxes_masks_loss: 0.5745 - val_loss: 3.3349 - val_regression_loss: 2.2157 - val_classification_loss: 0.5587 - val_boxes_masks_loss: 0.5605\n",
      "Epoch 10/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.4778 - regression_loss: 2.3362 - classification_loss: 0.5708 - boxes_masks_loss: 0.5708\n",
      "Epoch 00010: val_loss improved from 3.33488 to 3.29542, saving model to /data/models/mrcnn_model.h5\n",
      "637 instances of class 0 with average precision: 0.0313\n",
      "mAP: 0.0313\n",
      "21/21 [==============================] - 16s 766ms/step - loss: 3.4630 - regression_loss: 2.3263 - classification_loss: 0.5663 - boxes_masks_loss: 0.5704 - val_loss: 3.2954 - val_regression_loss: 2.2127 - val_classification_loss: 0.5226 - val_boxes_masks_loss: 0.5602\n"
     ]
    }
   ],
   "source": [
    "loss_history = model.fit_generator(\n",
    "    train_data,\n",
    "    steps_per_epoch=train_data.x.shape[0] // batch_size,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=val_data.x.shape[0] // batch_size,\n",
    "    callbacks=training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepcell.training import train_model_retinanet\n",
    "\n",
    "# model = train_model_retinanet(\n",
    "#     model=model,\n",
    "#     backbone=backbone,\n",
    "#     dataset=DATA_FILE,  # full path to npz file\n",
    "#     model_name='retinanet',\n",
    "#     sigma=3.0,\n",
    "#     alpha=0.25,\n",
    "#     gamma=2.0,\n",
    "#     include_masks=True,  # include mask generation -> MaskRCNN\n",
    "#     weighted_average=True,\n",
    "#     score_threshold=0.01,\n",
    "#     iou_threshold=0.5,\n",
    "#     max_detections=100,\n",
    "#     test_size=test_size,\n",
    "#     optimizer=optimizer,\n",
    "#     batch_size=batch_size,\n",
    "#     n_epoch=n_epoch,\n",
    "#     log_dir=LOG_DIR,\n",
    "#     model_dir=MODEL_DIR,\n",
    "#     lr_sched=lr_sched,\n",
    "#     rotation_range=180,\n",
    "#     flip=True,\n",
    "#     shear=False,\n",
    "#     zoom_range=(0.8, 1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Number: 899\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3b2504bcd494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from deepcell.utils.plot_utils import draw_detections\n",
    "\n",
    "index = np.random.randint(low=0, high=X_test.shape[0])\n",
    "print('Image Number:', index)\n",
    "\n",
    "image, mask = X_test[index:index + 1], y_test[index:index + 1]\n",
    "\n",
    "boxes, scores, labels = prediction_model.predict(image)\n",
    "\n",
    "image = 0.01 * np.tile(np.expand_dims(image[0, ..., 0], axis=-1), (1, 1, 3))\n",
    "mask = np.squeeze(mask)\n",
    "\n",
    "# copy to draw on\n",
    "draw = image.copy()\n",
    "\n",
    "# draw the detections\n",
    "draw_detections(draw, boxes[0], scores[0], labels[0],\n",
    "                label_to_name=lambda x: 'cell',\n",
    "                score_threshold=0.5,)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap='jet')\n",
    "ax[0].set_title('Source Image')\n",
    "\n",
    "ax[1].imshow(mask, cmap='jet')\n",
    "ax[1].set_title('Labeled Data')\n",
    "\n",
    "ax[2].imshow(draw, cmap='jet')\n",
    "ax[2].set_title('Detections')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
